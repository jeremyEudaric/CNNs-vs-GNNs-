{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b43d3c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import pickle\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torchvision.models as models\n",
    "from torch.nn import Module, Dropout\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.utils.data import Dataset, DataLoader,TensorDataset,random_split,SubsetRandomSampler, ConcatDataset\n",
    "import torchvision\n",
    "\n",
    "#from pytorchtools import EarlyStopping\n",
    "\n",
    "\n",
    "\n",
    "from torchvision import datasets, models, transforms\n",
    "import time\n",
    "import logging\n",
    "\n",
    "#from sklearn.metrics import mean_squared_error\n",
    "#from sklearn.model_selection import KFold\n",
    "#from sklearn.model_selection import train_test_split\n",
    "#from sklearn.model_selection import GridSearchCV\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e0958986",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transforms = transforms.Compose([\n",
    "                                    transforms.Resize(224),\n",
    "                                    transforms.RandomVerticalFlip(),\n",
    "                                    transforms.RandomHorizontalFlip(),\n",
    "                                    transforms.ColorJitter(brightness=0, contrast=0, saturation=1, hue=.5),\n",
    "                                    transforms.ToTensor()])\n",
    "\n",
    "\n",
    "val_transforms= transforms.Compose([\n",
    "                                    transforms.Resize(224),\n",
    "                                    transforms.ToTensor()])\n",
    "\n",
    "\n",
    "\n",
    "# cross validaion with 5 folders on the train \n",
    "train_dataset = datasets.ImageFolder('/Users/eudari01/GNNs_Vs_CNNs/tiles_CIN/train/', train_transforms) \n",
    "\n",
    "# cross validaion with 5 folders on the val \n",
    "val_dataset = datasets.ImageFolder('/Users/eudari01/GNNs_Vs_CNNs/tiles_CIN/val/', val_transforms) \n",
    "\n",
    "\n",
    "batch_size = 48\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, num_workers=16, shuffle=True)\n",
    "valid_loader= torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, num_workers=16, shuffle=True)\n",
    "\n",
    "                         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "43df0dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#chesk the classes \n",
    "#train_dataset.classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d7a3e5e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = torch.hub.load('pytorch/vision:v0.10.0', 'vgg19_bn', pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "446a5543",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ce4e4c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGG_19_Regression(Module):\n",
    "    \n",
    "    def __init__(self,):\n",
    "        \n",
    "        model = torch.hub.load('pytorch/vision:v0.10.0', 'vgg19_bn', pretrained=True)\n",
    "        \n",
    "        super(VGG_19_Regression,self).__init__()\n",
    "        self.model= model\n",
    "        self.Regression = nn.Linear(1000,1)\n",
    "        self.dropout = Dropout(0.5)\n",
    "        \n",
    "           \n",
    "         \n",
    "    def forward(self, out):\n",
    "        \n",
    "        x= self.model(out)\n",
    "        \n",
    "        x = x.view(x.size(0), -1) # flatten\n",
    "        \n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x =self.Regression(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "76cea0df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#Model = models.vgg19(pretrained=True).features.to(device).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7bfd92fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /Users/eudari01/.cache/torch/hub/pytorch_vision_v0.10.0\n"
     ]
    }
   ],
   "source": [
    "Model= VGG_19_Regression()\n",
    "\n",
    "MSE = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(Model.parameters(), lr=0.00001,momentum=0.9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "44c8ac1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_accuracy=[]\n",
    "train_loss = []\n",
    "val_loss = []\n",
    "\n",
    "def training(epoch):\n",
    "      \n",
    "    for i in range(5) : \n",
    "     \n",
    "        \n",
    "      \n",
    "        epoch_loss = 0.0\n",
    "        running_loss_train=0.0\n",
    "   \n",
    "        for data, label in tqdm(train_loader):\n",
    "        \n",
    "            label= label.to(torch.float32)\n",
    "            label = label.unsqueeze(1)\n",
    "\n",
    "         \n",
    "        #Transfer Data to GPU if available\n",
    "        #if torch.cuda.is_available():\n",
    "            #data = data.cuda()\n",
    "            #label = label.cuda()\n",
    "            #Model.train()\n",
    "            output = Model(data)\n",
    "            #you can change the loss and use MSE or Cross Entropy\n",
    "            loss = MSE(output,label)\n",
    "           \n",
    "            # Clear the gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            #Calculate accuracy \n",
    "            #prediction_train=output.argmax(dim=1)\n",
    "           # acc = (prediction_train== label1).float().mean()\n",
    "        \n",
    "            running_loss_train =+ loss.item()* data.size(0) \n",
    "            #Epoch\n",
    "            #epoch_accuracy += acc.item() / len(train_loader)\n",
    "            epoch_loss += running_loss_train / len(train_loader)\n",
    "            \n",
    "            \n",
    "        Model.eval()\n",
    "        epoch_val_loss = 0.0\n",
    "        running_loss_val= 0.0\n",
    "        \n",
    "  \n",
    "        \n",
    "        for data, label in valid_loader:\n",
    "            #if torch.cuda.is_available():\n",
    "                \n",
    "                #data = data.cuda()\n",
    "                #label = label.cuda()\n",
    "                #label= label.to(torch.float32)\n",
    "                #label = label.unsqueeze(1)\n",
    "    \n",
    "                output = Model(data)\n",
    "                \n",
    "                loss = MSE(output, label)\n",
    "                \n",
    "                # Clear the gradients\n",
    "                #optimizer.zero_grad()\n",
    "                \n",
    "                #val_loss.backward()\n",
    "                \n",
    "                #optimizer.step()\n",
    "                #Calculate accuracy \n",
    "                #prediction_val= val_output.argmax(dim=1)\n",
    "                \n",
    "                #mse_val=mean_squared_error(label,prediction_val)\n",
    "                \n",
    "                #val_acc = (prediction_val == label).float().mean()\n",
    "                \n",
    "                running_loss_val =+ loss.item()* data.size(0) \n",
    "               \n",
    "                #Epoch\n",
    "                #epoch_val_accuracy += val_acc.item() / len(valid_loader)\n",
    "                epoch_val_loss += running_loss_val / len(valid_loader)\n",
    "\n",
    "                    \n",
    "            \n",
    "        val_loss.append(epoch_val_loss)\n",
    "        train_loss.append(epoch_loss)\n",
    "    #train_accuracy.append(epoch_accuracy)\n",
    "        #print('Split: {}'.format(i))                \n",
    "        print(\n",
    "            f\"Epoch : {epoch+1} - train_loss : {epoch_loss:.4f} - val_loss : {epoch_val_loss:.4f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9440fd81",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 61/61 [17:18<00:00, 17.03s/it]\n",
      "/Users/eudari01/anaconda3/lib/python3.9/site-packages/torch/nn/modules/loss.py:520: UserWarning: Using a target size (torch.Size([48])) that is different to the input size (torch.Size([48, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/eudari01/anaconda3/lib/python3.9/site-packages/torch/nn/modules/loss.py:520: UserWarning: Using a target size (torch.Size([7])) that is different to the input size (torch.Size([7, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "  0%|                                                    | 0/61 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 2 - train_loss : 198.5318 - val_loss : 104.6466 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 61/61 [16:49<00:00, 16.55s/it]\n",
      "  0%|                                                    | 0/61 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 2 - train_loss : 104.3880 - val_loss : 101.2743 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 61/61 [16:47<00:00, 16.52s/it]\n",
      "  0%|                                                    | 0/61 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 2 - train_loss : 102.6392 - val_loss : 93.1937 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 61/61 [16:51<00:00, 16.58s/it]\n",
      "  0%|                                                    | 0/61 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 2 - train_loss : 100.5850 - val_loss : 95.7866 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 61/61 [16:51<00:00, 16.59s/it]\n",
      "  0%|                                                    | 0/61 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 2 - train_loss : 99.0143 - val_loss : 94.2361 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 61/61 [16:50<00:00, 16.56s/it]\n",
      "  0%|                                                    | 0/61 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 3 - train_loss : 98.7961 - val_loss : 104.8711 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 61/61 [16:54<00:00, 16.64s/it]\n",
      "  0%|                                                    | 0/61 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 3 - train_loss : 98.2802 - val_loss : 91.5123 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 61/61 [16:56<00:00, 16.66s/it]\n",
      "  0%|                                                    | 0/61 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 3 - train_loss : 97.1217 - val_loss : 93.2084 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 61/61 [16:55<00:00, 16.65s/it]\n",
      "  0%|                                                    | 0/61 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 3 - train_loss : 96.7735 - val_loss : 91.4922 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 61/61 [16:53<00:00, 16.61s/it]\n",
      "  0%|                                                    | 0/61 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 3 - train_loss : 97.1342 - val_loss : 91.1122 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 61/61 [16:50<00:00, 16.57s/it]\n",
      "  0%|                                                    | 0/61 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 4 - train_loss : 97.3862 - val_loss : 93.6766 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 61/61 [16:55<00:00, 16.65s/it]\n",
      "  0%|                                                    | 0/61 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 4 - train_loss : 97.6438 - val_loss : 91.8514 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 61/61 [16:48<00:00, 16.54s/it]\n",
      "  0%|                                                    | 0/61 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 4 - train_loss : 96.4948 - val_loss : 90.1754 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 61/61 [16:51<00:00, 16.59s/it]\n",
      "  0%|                                                    | 0/61 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 4 - train_loss : 97.0111 - val_loss : 91.9637 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 61/61 [16:40<00:00, 16.41s/it]\n",
      "  0%|                                                    | 0/61 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 4 - train_loss : 95.9387 - val_loss : 91.5774 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 61/61 [16:36<00:00, 16.33s/it]\n",
      "  0%|                                                    | 0/61 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 5 - train_loss : 96.6626 - val_loss : 92.1217 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 61/61 [16:33<00:00, 16.28s/it]\n",
      "  0%|                                                    | 0/61 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 5 - train_loss : 96.1591 - val_loss : 92.9518 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 61/61 [16:26<00:00, 16.17s/it]\n",
      "  0%|                                                    | 0/61 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 5 - train_loss : 96.0044 - val_loss : 90.3634 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 61/61 [16:10<00:00, 15.91s/it]\n",
      "  0%|                                                    | 0/61 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 5 - train_loss : 96.0132 - val_loss : 91.3176 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 61/61 [15:56<00:00, 15.68s/it]\n",
      "  0%|                                                    | 0/61 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 5 - train_loss : 95.1435 - val_loss : 89.9238 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 61/61 [15:59<00:00, 15.73s/it]\n",
      "  0%|                                                    | 0/61 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 6 - train_loss : 95.8815 - val_loss : 89.8418 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 61/61 [15:59<00:00, 15.73s/it]\n",
      "  0%|                                                    | 0/61 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 6 - train_loss : 97.1110 - val_loss : 90.6401 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 61/61 [16:05<00:00, 15.84s/it]\n",
      "  0%|                                                    | 0/61 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 6 - train_loss : 96.0611 - val_loss : 91.0066 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 61/61 [16:03<00:00, 15.80s/it]\n",
      "  0%|                                                    | 0/61 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 6 - train_loss : 95.0228 - val_loss : 90.3243 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 61/61 [16:08<00:00, 15.88s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 6 - train_loss : 96.3428 - val_loss : 91.4109 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "epochs=5\n",
    "for epoch in range(1,epochs+1):\n",
    "     training(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e442022e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# to check the overfiting \n",
    "plt.title(\"Training and Validation Loss\")\n",
    "plt.plot(train_loss, 'bo-',label=\"train\")\n",
    "plt.plot(val_loss, 'ro-',label=\"val\")\n",
    "plt.xlabel(\"iterations\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.savefig\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "id": "20f23878",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = transforms.Compose([\n",
    "                                  transforms.Resize((224,224)),\n",
    "                                  transforms.ToTensor(),\n",
    "                                  transforms.Normalize([0.5,0.5,0.5], [0.5,0.5,0.5]) \n",
    "                                  ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "id": "c1eb3463",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['MB-0028_train_metastasis_3328x_768y_256tilesize_68foregroundLevel.jpg',\n",
       " 'MB-0028_train_metastasis_3072x_256y_256tilesize_31foregroundLevel.jpg',\n",
       " 'MB-0028_train_metastasis_1280x_256y_256tilesize_63foregroundLevel.jpg',\n",
       " 'MB-0028_train_metastasis_1792x_512y_256tilesize_52foregroundLevel.jpg',\n",
       " 'MB-0028_train_metastasis_3072x_512y_256tilesize_45foregroundLevel.jpg',\n",
       " 'MB-0028_train_metastasis_768x_1024y_256tilesize_63foregroundLevel.jpg',\n",
       " 'MB-0028_train_metastasis_2304x_0y_256tilesize_61foregroundLevel.jpg',\n",
       " 'MB-0028_train_metastasis_1792x_256y_256tilesize_54foregroundLevel.jpg',\n",
       " 'MB-0028_train_metastasis_2304x_512y_256tilesize_34foregroundLevel.jpg',\n",
       " 'MB-0028_train_metastasis_2304x_256y_256tilesize_24foregroundLevel.jpg',\n",
       " 'MB-0028_train_metastasis_3584x_768y_256tilesize_44foregroundLevel.jpg',\n",
       " 'MB-0028_train_metastasis_2816x_256y_256tilesize_65foregroundLevel.jpg',\n",
       " 'MB-0028_train_metastasis_768x_1280y_256tilesize_63foregroundLevel.jpg',\n",
       " 'MB-0028_train_metastasis_2048x_256y_256tilesize_35foregroundLevel.jpg',\n",
       " 'MB-0028_train_metastasis_2048x_512y_256tilesize_36foregroundLevel.jpg',\n",
       " 'MB-0028_train_metastasis_1280x_512y_256tilesize_50foregroundLevel.jpg',\n",
       " 'MB-0028_train_metastasis_2560x_256y_256tilesize_69foregroundLevel.jpg']"
      ]
     },
     "execution_count": 311,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = ('/Users/eudari01/GNNs_Vs_CNNs/tiles_CIN/train/tiles_has_CIN_HE_trainsetCV_fold_0/MB-0028/train_metastasis') \n",
    "images= os.listdir(('/Users/eudari01/GNNs_Vs_CNNs/tiles_CIN/train/tiles_has_CIN_HE_trainsetCV_fold_0/MB-0028/train_metastasis') )\n",
    "images\n",
    "#pred_path = '/content/drive/MyDrive/Chest X-ray (Covid-19 & Pneumonia)/Test_images/Data/'\n",
    "#test_imgs = glob.glob(dataset+'/*')\n",
    "#loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, num_workers=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "id": "dd370d21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/Users/eudari01/GNNs_Vs_CNNs/tiles_CIN/train/tiles_has_CIN_HE_trainsetCV_fold_0/MB-0028/train_metastasis']"
      ]
     },
     "execution_count": 302,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "id": "5ff9e6b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x7fef446f1f40>"
      ]
     },
     "execution_count": 288,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "id": "28ca6cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = datasets.ImageFolder('/Users/eudari01/GNNs_Vs_CNNs/tiles_CIN/train/tiles_has_CIN_HE_trainsetCV_fold_0/MB-0028/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "id": "57d2d249",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (2776085356.py, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipykernel_434160/2776085356.py\"\u001b[0;36m, line \u001b[0;32m3\u001b[0m\n\u001b[0;31m    images,labels = next(iter(dataset)\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "\n",
    " \n",
    "    #for eachtile in images:\n",
    "\n",
    "        images,labels = next(iter(dataset)\n",
    "    \n",
    "        \n",
    "        \n",
    "        outputs=Model(eachtile)\n",
    "        \n",
    "        predicted = outputs.data\n",
    "        # created a tensor\n",
    "        predicted=torch.Tensor(predicted)\n",
    "        # numpy array as list \n",
    "        predicted=predicted.numpy()\n",
    "        # save the numpy array  \n",
    "        np.savetxt('test.out', predicted, delimiter=',') \n",
    "    \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f84d4c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "id": "e116ca2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#images, labels = next(iter(dataset )\n",
    "#image_tensor = dataset.unsqueeze_(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "ee91cf90",
   "metadata": {},
   "outputs": [],
   "source": [
    "p=('/Users/eudari01/GNNs_Vs_CNNs/tiles_CIN/train/tiles_has_CIN_HE_trainsetCV_fold_0/MB-0028/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "20d45bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "f23277fa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "tuple indices must be integers or slices, not tuple",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_434160/1266920771.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mprediction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_434160/75509894.py\u001b[0m in \u001b[0;36mprediction\u001b[0;34m(Image)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0moutputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mImage\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m...\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0mpredicted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mpredicted\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredicted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: tuple indices must be integers or slices, not tuple"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "fd1227db",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_434160/691099581.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m256\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mitem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtrue_target\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m...\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    229\u001b[0m             \u001b[0mtuple\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0mwhere\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mclass_index\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;32mclass\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m         \"\"\"\n\u001b[0;32m--> 231\u001b[0;31m         \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    232\u001b[0m         \u001b[0msample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "index = 256\n",
    "item = dataset[index]\n",
    "image = item[0]\n",
    "true_target = item[1]\n",
    "prediction = Model(image[None, ...].float())\n",
    "#np.savetxt('output.csv', arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "91e39711",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'save'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_434160/2584191970.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/Users/eudari01/GNNs_Vs_CNNs/prediction/Patient_ID_prediction.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'save'"
     ]
    }
   ],
   "source": [
    "p=prediction.detach().numpy()\n",
    "\n",
    "p.save('/Users/eudari01/GNNs_Vs_CNNs/prediction/Patient_ID_prediction.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "9358987b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-24 10:52:36.811337: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-06-24 10:52:36.811378: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-06-24 10:52:36.811401: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (895wy03): /proc/driver/nvidia/version does not exist\n",
      "2022-06-24 10:52:36.813424: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Attempt to convert a value (tensor([1.7689], grad_fn=<MaxBackward0>)) with an unsupported type (<class 'torch.Tensor'>) to a Tensor.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_434160/3993271990.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultiply\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mpredicted\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36mconvert_to_eager_tensor\u001b[0;34m(value, ctx, dtype)\u001b[0m\n\u001b[1;32m    100\u001b[0m       \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_datatype_enum\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m   \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEagerTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Attempt to convert a value (tensor([1.7689], grad_fn=<MaxBackward0>)) with an unsupported type (<class 'torch.Tensor'>) to a Tensor."
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tensor = tf.multiply( predicted, 42)\n",
    "tensor.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "300434f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c932bf9f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5fea2169",
   "metadata": {},
   "outputs": [],
   "source": [
    "Path_patient_ID= ('/Users/eudari01/GNNs_Vs_CNNs/tiles_CIN/train/tiles_has_CIN_HE_trainsetCV_fold_0/MB-0028/train_metastasis'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "aa9a270c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2.2784]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction = Model(image[None, ...].float())\n",
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "30afba77",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() takes 1 positional argument but 2 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_434160/531919211.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVGG_19_Regression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPath_patient_ID\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#prediction.save('/Users/eudari01/GNNs_Vs_CNNs/prediction/Patient_ID_prediction.txt')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() takes 1 positional argument but 2 were given"
     ]
    }
   ],
   "source": [
    "\n",
    "        for eachtile in Path_patient_ID:\n",
    "            \n",
    "            Model.eval()\n",
    "            \n",
    "            for data, label in patient_valid_loader:\n",
    "                \n",
    "        \n",
    "            prediction = Model(eachtile)\n",
    "        \n",
    "            prediction.save('/Users/eudari01/GNNs_Vs_CNNs/prediction/Patient_ID_prediction.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3ec57f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/Users/eudari01/GNNs_Vs_CNNs/prediction/'\n",
    "  \n",
    "    # Specify the file name\n",
    "file = 'Patient_ID_prediction.txt'\n",
    "  \n",
    "    # Before creating\n",
    "dir_list = os.listdir(path) \n",
    "    \n",
    "  \n",
    "    # Creating a file at specified location\n",
    "with open(os.path.join(path, file), 'w') as fp:\n",
    "    \n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b81db0af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def patientprediction (Model, Path_patient_ID) :\n",
    "    \n",
    "    def __init__(self):\n",
    "    \n",
    "        Model.eval()\n",
    "    \n",
    "        path = '/Users/eudari01/GNNs_Vs_CNNs/prediction/'\n",
    "  \n",
    "    # Specify the file name\n",
    "        file = 'Patient_ID_prediction.txt'\n",
    "  \n",
    "    # Before creating\n",
    "        dir_list = os.listdir(path) \n",
    "    \n",
    "  \n",
    "    # Creating a file at specified location\n",
    "        with open(os.path.join(path, file), 'w') as fp:\n",
    "    \n",
    "            pass\n",
    "\n",
    "        for eachtile in Path_patient_ID: \n",
    "        \n",
    "            prediction = Model(eachtile)\n",
    "        \n",
    "            prediction.save('/Users/eudari01/GNNs_Vs_CNNs/prediction/Patient_ID_prediction.txt')\n",
    "        \n",
    "        return \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e73c65c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "Path_patient_ID= ('/Users/eudari01/GNNs_Vs_CNNs/tiles_CIN/train/tiles_has_CIN_HE_trainsetCV_fold_0/MB-0028')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4d37ebee",
   "metadata": {},
   "outputs": [],
   "source": [
    "patientprediction (VGG_19_Regression, Path_patient_ID) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef66be7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f395f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Patient MSE (path_for_all_patients):\n",
    "    df_MSE = pd.DataFrame()\n",
    "    \n",
    "    for eachpatient in path_for_all_patients:\n",
    "        \n",
    "        file = open('/Users/eudari01/GNNs_Vs_CNNs/prediction/Patient_ID_prediction.txt', 'w')\n",
    "        \n",
    "        # Read a text file to a dataframe using colon delimiters\n",
    "        student_csv =  pd.read_csv('students.txt', sep=':', engine='python')\n",
    " \n",
    "        print(student_csv)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c4d5ab7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
